{
  
    
        "post0": {
            "title": "Converting CRAFT to TFLite: A Guide to PyTorch-TFLite Conversion",
            "content": "This is an end-to-end tutorial on how to convert a PyTorch model to TensorFlow Lite (TFLite) using ONNX. Specifically, we will be using the CRAFT model (proposed in this paper) which is essentially a text detector. Above is the overview of what’s covered in the tutorial - . Please open the notebooks included in this repository and follow along with this blog post. You may also directly download already converted TFLite Models from this repository. . Brief Overview of the CRAFT Model . Character Region Awareness for Text Detection in short CRAFT was proposed in this paper and is known for its efficiency as well as precise performance The main principle of CRAFT is to localize the individual character regions and link the detected characters to text instances. . CRAFT produces two scores for each character-character region score and affinity score. . Character Region Score is used to localize the individual character | Affinity Score is used to group each character into a single instance. | . . As we all know in most of the image detectors use VGG16 as feature extractor CRAFT is not an exception for it and for decoding the architecture is similar to UNet. . . The above diagram is taken from the original paper . TFLite Conversion Flow . Currently, the integer quantization is erroring out and it has been reported to the TensorFlow Lite team. . Update from TFLite team: Currently support for NCHW image format(like those converted from PyTorch) is quite limited at this moment, which caused this issue with full integer quantized model. . You can find the full reply from TensorFlow Lite team here . Clove AI team already provided pre-trained weights we can use for making inference on images. But the framework(PyTorch) is not ideal for mobile applications and also for low latency devices like Raspberry Pi and Fully Integer Devices like Google Coral and MicroControllers. . TensorFlow Lite is a framework that is well suited for running Deep Learning Models on edge devices and mobile devices. Now a days usage of edge devices become popular mainly due to 3 reasons . Lower Latency | No requirement of Internet | Privacy Protection | . This is why we first convert these pre-trained weights to TFLite which would be more suitable for low latency devices and mobile applications. . PyTorch Model to ONNX Model . Refer this notebook for complete code details mentioned in this section. . Open Neural Network Exchange in short ONNX is an open format built to represent machine learning models. The best thing about ONNX is interoperability. You can develop in your preferred framework without worrying about downstream inference applications. Exporting the models to ONNX format requires some mandatory parameters: . Pre-trained Model | Sample Input | Path to save the model | Input and Output Node names | torch.onnx.export(pytorch_model, x, &#39;craft.onnx&#39;, opset_version=10, do_constant_folding=True, input_names=[&#39;input&#39;], output_names=[&#39;output&#39;], dynamic_axes= shape_dict) . Here are some details about the above code snippet - . export() function executes the model and records a trace of operators that are used to compute output. . | To execute the model we need to provide the input. This value can be random as long as type and dimensions are matched because the export function just runs the model to trace the operators that are being used to compute output. . | Exported ONNX model will be of fixed dimension unless specified in the dynamic_axes parameter. In the above code we specified batch_size, width and height of the image are dynamic and the channels which are not specified in the dynamic_axes will be fixed according to input dimension. . | To visualize the exported onnx model you can use this tool. . | . Once the model is exported, load the model and verify the model structure and confirm whether the model has a valid schema or not. . The below code snippet checks whether the exported onnx model has a valid schema. . onnx_model = onnx.load(&#39;craft.onnx&#39;) onnx.checker.check_model(onnx_model) . Expected Output: . Raises Runtime Error if model is not valid. If valid no output. . Compare ONNX output with Pytorch Model Output: . To check whether the exported ONNX model was faulty or not follow these steps: . Create a Sample Input | Run pre-trained Pytorch Model and save output | Run exported ONNX model and save output | Compare both pytorch output and ONNX model output. | . Below is the code snippet required to implement the above steps: . ort_session = onnxruntime.InferenceSession(&#39;craft.onnx&#39;) ort_inputs = {ort_session.get_inputs()[0].name:onnx_runtime_input} ort_outs = ort_session.run(None, ort_inputs) np.testing.assert_allclose(pytorch_out, ort_outs[0], rtol=1e-03, atol=1e-05) . The above code snippet compares both pytorch model output and onnx model output and errors out if the outputs are not matched with the tolerance mentioned. . It compares the difference between pytorch output and onnx output to atol+rtol*abs(onnx output) . You can refer to this documentation for more details about this function. If the ONNX conversion was faulty then the assertion statement would have errored out. . Great! We converted to ONNX. . ONNX Model to TensorFlow SavedModel . Refer to this notebook for complete code details mentioned in this section. . As mentioned earlier, the best feature of ONNX is interoperability. Once we have the access to the ONNX model we can convert it into any other existing popular frameworks very easily. . Let’s see how to convert the ONNX model to the TensorFlow SavedModel. . import onnx from onnx_tf.backend import prepare onnx_model = onnx.load(&#39;craft.onnx&#39;) tf_rep = prepare(onnx_model) tf_rep.export_graph(&#39;craft_tf_graph&#39;) . After exporting to TensorFlow graphs we can inspect the graph using the same tool which we used to visualize the onnx model. . Note: Please refer to the installation instructions for validity of onnx and onnx_tf versions. . A SavedModel contains all the information about the TensorFlow program, along with weights and computation. As we don’t require any extra code to build the model it is very easy to share or deploy TensorFlow saved models. . The file structure of SavedModel craft_tf_graph will be as follows: . craft_tf_graph |- saved_model.pb |- assets |- variables |- variables.data-00000-of-00001 |- variables.index . The saved_model.pb contains an actual model and set of named signatures each identifying a function that accepts input tensors and produces output tensors. . The variables directory contains standard checkpoints and assets directory contains files used by tensorflow graph. assets directory is unused in this example as saved model has no requirement of extra files. . To know more about TensorFlow SavedModel please refer to this guide. . You can load the saved model assuming it is Keras saved model. Below is the code snippet to load the saved model: . model = tf.keras.models.load_model(&#39;craft_tf_graph&#39;) # or model = tf.saved_model.load(&#39;craft_tf_graph&#39;) . You can easily convert to the TFLite Model easily from the saved model. But inorder to change any input dimension you can set it by loading the concrete function from the saved model. . Below is the code snippet to set the input shape required for the TFLite format. . concrete_func = model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY] concrete_func.inputs[0].set_shape([None, 3, 800, 600]) . TensorFlow SavedModel to TFLite . To convert a TensorFlow model into TensorFlow Lite model can be done from 3 ways: . From Saved Model | From Keras Model | From Concrete Function | . You can refer to this blog for various conversion techniques. We will convert to TFLite from concrete function. . Below is the code snippet to load the concrete function into the TFLiteConverter. . converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func]) . While converting to TFLite we can choose several quantization methods. Refer to this guide for various Post training Quantization techniques. . Dynamic Range Quantization | Float16 Quantization | . Dynamic Range Quantization . Default Optimization is Dynamic Range Quantization. . converter.optimizations = [tf.lite.Optimize.DEFAULT] . Float16 Quantization . For Float16 all other things remain same we just need to add this line . converter.target_spec.supported_types = [tf.float16] . Convert and Store the Model: . tf_lite_model = converter.convert() open(&#39;craft.tflite&#39;, &#39;wb&#39;).write(tf_lite_model) . Quantization Type Model Size . Dynamic Range | 20MB | . Float16 | 40MB | . Original PyTorch model size is around 80MB . Running inference with TFLite Models . Refer to this notebook for complete code details mentioned in this section. . Once the TFLite models are generated we need to make sure they are working as expected. So let’s do inference on the real image and check the output. . Run the preprocessing steps mentioned in this notebook before feeding to the tflite model. . Below is the code snippet to run the inference with TFLite model. . interpreter = tf.lite.Interpreter(model_path=&#39;craft.tflite&#39;) interpreter.allocate_tensors() input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() input_shape = input_details[0][&#39;shape&#39;] interpreter.set_tensor(input_details[0][&#39;index&#39;], input_data) interpreter.invoke() y = interpreter.get_tensor(output_details[0][&#39;index&#39;]) feature = interpreter.get_tensor(output_details[1][&#39;index&#39;]) . After the post-processing steps mentioned in this notebook the output image (with dynamic range quantized model) would look like this alongside with the output of the original model output. . Results . Output with Dynamic Range Quantized Model: . . Output with Float16 Quantized Model: . . It is clearly evident that the results of Float16 quantized model are better than results of Dynamic Range quantized model but at the cost of model size. . Conclusion . In this post we have covered all the steps required to convert any PyTorch pre-trained model to TFLite format. If you want to use the same notebook for all of the mentioned steps you can use this notebook. . Wondering about how the CRAFT model would perform in the mobile device? Refer to this blog post that compares the CRAFT model with the EAST model w.r.t. many useful metrics such as memory, inference latency, performance and so on. . Acknowledgments: . Thanks to Sayak Paul , Le Viet Gia Khanh(from TFLite team) for their constant guidance. .",
            "url": "https://tulasi.dev/craft-in-tflite",
            "relUrl": "/craft-in-tflite",
            "date": " • Nov 24, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am currently working at Dynamic Digital Technology(acquired by Motorola Solutions) on Computer Vision, OCR and Information Extraction from Documents. My projects mainly focus on developing low latency models to be deployed in mobile devices or edge devices. . Before to that I worked in Mihup where I worked on Text to Speech and Speech Recognition Projects for edge devices. . In free time I like contributing to open-source recently started publishing models to Tensorflow Hub . Work Experience: . Deep Learning Engineer, Dynamic Digital Technology (Apr 2020- Present) | Machine Learning Engineer, Mihup (June 2019 - Feb 2020) | Intern, Applied AI Course (Dec 2018 - Mar 2019) | .",
          "url": "https://tulasi.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tulasi.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}